{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnf7uFR8CPy+Zp8n37PJ3F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install numpy"
      ],
      "metadata": {
        "id": "SjaKhDA3Aeuy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "wWV9-2TgVg9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "import math"
      ],
      "metadata": {
        "id": "4yDsO-3fAdQG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Neural Network Class"
      ],
      "metadata": {
        "id": "_CfX12l3VjVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Currently only implements a linear network structure\n",
        "# input_shape does not include batch size\n",
        "# input_shape needs to be refactored.  Currently takes an int\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        input_shape = self.layers[-1].output_shape if len(self.layers) > 0 else self.input_shape \n",
        "        layer.build(input_shape=input_shape)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def set_loss_function(self, loss_func, loss_deriv):\n",
        "        self.loss_func = loss_func\n",
        "        self.loss_deriv = loss_deriv\n",
        "\n",
        "    # Lacks error checking on input dimensions\n",
        "    def train(self, X, Y, lr, epochs, batch_size=None):\n",
        "        num_samples = X.shape[0]\n",
        "        if batch_size is None or batch_size > num_samples:\n",
        "            batch_size = num_samples\n",
        "        num_batches = math.ceil(num_samples / batch_size)\n",
        "\n",
        "        # NOTE: this method is quick and dirty and will only work for a linear network\n",
        "        for epoch in range(epochs):\n",
        "            error = 0\n",
        "\n",
        "            # Should probably shuffle batches(or samples?)...\n",
        "            for i in range(0, num_batches, 1):\n",
        "                X_batch = X[batch_size*i : batch_size*(i+1)]\n",
        "                Y_batch = Y[batch_size*i : batch_size*(i+1)]\n",
        "                \n",
        "                output = X_batch\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_prop(output)\n",
        "\n",
        "                Y_batch = Y_batch.reshape(output.shape)\n",
        "\n",
        "                # Compute reported error(loss)\n",
        "                # In Keras, reported losses are the average of per sample losses in each batch\n",
        "                # Assumption: error function returns avg error of samples within batch\n",
        "                # Multiply by number of samples in batch, then later divide by total number of samples\n",
        "                # This accounts for variable batch size\n",
        "                error += self.loss_func(output, Y_batch) * X_batch.shape[0]\n",
        "\n",
        "                error_gradient = self.loss_deriv(output, Y_batch)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error_gradient = layer.backprop(error_gradient)\n",
        "\n",
        "                # Update using the computed weight gradients\n",
        "                for layer in self.layers:\n",
        "                    layer.update(lr)\n",
        "\n",
        "            # Divide total error by number of samples for per-sample mean error\n",
        "            error /= len(X)\n",
        "            \n",
        "            print(\"Epoch {:d}: {:f}\".format(epoch, error))\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = X\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward_prop(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XKt02umYAhxt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Layer Class"
      ],
      "metadata": {
        "id": "0ysiZQ1mI1bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(ABC):\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "\n",
        "    # Used to set expected input, output dimensions once adjacent layers are known,\n",
        "    # as well as construct weight matrices\n",
        "    @abstractmethod\n",
        "    def build(self, input_shape=None, output_shape=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward_prop(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backprop(self, error, lr):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def update(self, lr):\n",
        "        return\n",
        "\n"
      ],
      "metadata": {
        "id": "vWIAYxsZZH1g"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully Connected(Dense) Layer"
      ],
      "metadata": {
        "id": "dkG0AwQxJGEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# aka Dense Layer\n",
        "class FullyConnectedLayer(Layer):\n",
        "    def __init__(self, neurons, input_shape=None, weight_range=(-0.5,0.5)):\n",
        "        super().__init__()\n",
        "        self.neurons = neurons\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = neurons\n",
        "        self.weight_range = weight_range\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.grad_weights = None\n",
        "        self.bias_weights = None\n",
        "        self.num_samples_used = 0\n",
        "\n",
        "    def build(self, input_shape=None, output_shape=None):\n",
        "        # Output shape is equal to neurons for standard dense layer\n",
        "        self.input_shape = input_shape or self.input_shape\n",
        "        self.init_weights()\n",
        "\n",
        "    # Weight initialization\n",
        "    #   See https://www.analyticsvidhya.com/blog/2021/05/how-to-initialize-weights-in-neural-networks/\n",
        "    #   -Small magnitude is recommended\n",
        "    #   -Heuristics are good\n",
        "    #   -Just randomizing with a range of 1\n",
        "    def init_weights(self):\n",
        "        min_w = self.weight_range[0]\n",
        "        max_w = self.weight_range[1]\n",
        "        self.weights = np.random.uniform(min_w, max_w, (self.input_shape, self.neurons))\n",
        "        self.bias = np.zeros((1, self.neurons))\n",
        "        # Matrices for summed weight gradients during backprop\n",
        "        # Used to store gradients for post-backprop GD update\n",
        "        self.grad_weights = np.zeros(self.weights.shape)\n",
        "        self.grad_bias = np.zeros(self.bias.shape)\n",
        "        # Stores number of samples adding to current gradient sum matrices\n",
        "        self.num_samples_used = 0  \n",
        "\n",
        "    def forward_prop(self, inputs):\n",
        "        # Track amount of samples included in this batch so far\n",
        "        # Required for averaging sum of sample gradients in update\n",
        "        self.num_samples_used += inputs.shape[0]\n",
        "\n",
        "        # Y = XW + B, where\n",
        "        #   X is vector of inputs\n",
        "        #   W is matrix of weights\n",
        "        #   B is column of biases\n",
        "        outputs = np.matmul(inputs, self.weights) + self.bias\n",
        "        self.inputs = inputs\n",
        "        return outputs\n",
        "\n",
        "    # dE_dY is of shape (1, neurons)\n",
        "    def backprop(self, dE_dY):\n",
        "        # Compute gradient\n",
        "        # Recall Y = XW + B, where\n",
        "        #   X is vector of inputs\n",
        "        #   W is matrix of weights\n",
        "        #   B is column of biases\n",
        "        # So, for a given weight wij(neuron i, weight from input j)\n",
        "        #   yi = xj*wij + bi\n",
        "        #   dE/dwij = dE/dyi * dyi/dwij             = dE/dyi * xj\n",
        "        #   dE/dbi  = dE/dyi * dyi/dbi = dE/dyi * 1 = dE/dyi\n",
        "        # (inputs, outputs) = (inputs, 1) . (1, outputs)\n",
        "        transpose = self.inputs.swapaxes(-1,-2)\n",
        "        dE_dW = np.matmul(transpose, dE_dY)\n",
        "        dE_dB = dE_dY  \n",
        "\n",
        "        # For each layer, have matrix of weight/bias derivatives matching weight dimensions\n",
        "        # Add onto it for each sample, then divide by batch size for avg deriv\n",
        "        self.grad_weights += np.sum(dE_dW, axis=0)\n",
        "        self.grad_bias += np.sum(dE_dB, axis=0)\n",
        "\n",
        "        # Pass along error gradient(dE_dX)\n",
        "        # Y(output) of previous layer is this layer's X(input)\n",
        "        #   dE/dxj = dE/dyi * dyi/dxj               = dE/dyi * wij\n",
        "        # (1, inputs) = (1, outputs) . (outputs, inputs)\n",
        "        dE_dX = np.dot(dE_dY, self.weights.T)\n",
        "        return dE_dX\n",
        "\n",
        "    def update(self, lr):\n",
        "        # Average summed weight gradients by dividing by number of samples in batch\n",
        "        self.grad_weights /= self.num_samples_used\n",
        "        self.grad_bias /= self.num_samples_used\n",
        "\n",
        "        # Update via gradient descent\n",
        "        self.weights  = self.weights - (lr * self.grad_weights)\n",
        "        self.bias = self.bias - (lr * self.grad_bias)\n",
        "\n",
        "        # Reset gradient sums, batch size count for next batch\n",
        "        self.grad_weights = np.zeros(self.weights.shape)\n",
        "        self.grad_bias = np.zeros(self.bias.shape)\n",
        "        self.num_samples_used = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "c1QL5cZh7iCC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Layer"
      ],
      "metadata": {
        "id": "zbxpdl-9It14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationLayer(Layer):\n",
        "    def __init__(self,activation_func, derivative_func):\n",
        "        super().__init__()\n",
        "        self.activation = activation_func\n",
        "        self.derivative = derivative_func\n",
        "\n",
        "    def build(self, input_shape=None, output_shape=None):\n",
        "        if input_shape is not None:\n",
        "            self.input_shape = input_shape\n",
        "        self.output_shape = self.input_shape\n",
        "\n",
        "    def forward_prop(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        outputs = self.activation(inputs)\n",
        "        return outputs\n",
        "\n",
        "    # dE_dY = (1, outputs)\n",
        "    # derivative(inputs) is another vector of (1, inputs)\n",
        "    # |inputs| = |outputs| since just applying function to each  \n",
        "    def backprop(self, dE_dY, lr):\n",
        "        dY_dX = self.derivative(self.inputs)\n",
        "        dE_dX = dE_dY * dY_dX\n",
        "        return dE_dX\n",
        "\n",
        "    def update(self, lr):\n",
        "        return\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "Rnybvh2O7QBR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activations"
      ],
      "metadata": {
        "id": "RmAu5PUZInBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def relu_d(x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_d(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "    # Equivalent to?\n",
        "    #return np.exp(-x) / np.power(1 + np.exp(-x), 2)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_d(x):\n",
        "    return 1 - np.power(np.tanh(x),2)\n",
        "\n",
        "# Temperature controls \"confidence\"\n",
        "# AKA low temperature(<1) means high values will be counted more strongly, small values even smaller\n",
        "# High temperature(>1) means everything is more similar\n",
        "def softmax(X, temp=1):\n",
        "    E = np.exp(X)\n",
        "    sum = np.sum(np.exp(X))\n",
        "    return E / sum\n",
        "\n",
        "def softmax_d(X):\n",
        "    return softmax(X)\n",
        "    #raise NotImplementedError"
      ],
      "metadata": {
        "id": "yYrY1vBV1F5-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Functions"
      ],
      "metadata": {
        "id": "SGb6nvQbJb91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "class LossFunction(ABC):\n",
        "    def __init__(self):\n",
        "        self.func = None\n",
        "        self.deriv = None  \n",
        "\n",
        "# Sum of squared errors for all samples divided by num samples\n",
        "# Will this work for batch?\n",
        "# https://stackoverflow.com/questions/55936214/correct-way-to-calculate-mse-for-autoencoders-with-batch-training\n",
        "def mse(y_pred, y_true):\n",
        "    #return np.mean(np.power(y_true - y_pred, 2))\n",
        "    return np.mean(np.mean(np.power(y_true - y_pred, 2), axis=1))\n",
        "\n",
        "def mse_d(y_pred, y_true):\n",
        "    return -2 * (y_true - y_pred) / y_true.size;\n",
        "\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def cross_entropy_d(y_pred, y_true):\n",
        "    raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "LcTB1lyPJeHF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Test"
      ],
      "metadata": {
        "id": "SxV8AWWWwLgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "\n",
        "#x_train = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "#y_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "#print(x_train.shape)\n",
        "#print(y_train.shape)\n",
        "\n",
        "model = NeuralNetwork(x_train.shape[-1]) #(x_train.shape[1:])\n",
        "model.add_layer(FullyConnectedLayer(3))\n",
        "model.add_layer(ActivationLayer(tanh, tanh_d))\n",
        "model.add_layer(FullyConnectedLayer(1))\n",
        "model.add_layer(ActivationLayer(tanh, tanh_d))\n",
        "model.set_loss_function(mse, mse_d)\n",
        "\n",
        "model.train(x_train, y_train, 0.1, 1000, batch_size=1)\n",
        "\n",
        "pred = model.predict(x_train)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "fCBUqLy9ByL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "tuj7-Jo-IvBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# X is of shape (samples, 28, 28) with each value being [0-255] (greyscale)\n",
        "# Y is of shape (samples,) with values [0-9]\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape to (samples, w*h), EXCEPT\n",
        "# Actually (samples, 1, w*h) for matrix multiplication reasons in first layer backprop\n",
        "# Should add a fix in NN class to handle this internally\n",
        "# Maybe InputLayer class\n",
        "x_train = x_train.reshape((x_train.shape[0], 1, np.prod(x_train.shape[1:]))) / 255\n",
        "x_test = x_test.reshape((x_test.shape[0], 1, np.prod(x_test.shape[1:]))) / 255\n",
        "\n",
        "# Normalization/scaling makes a big difference here(may depend on final layer)\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "act = sigmoid\n",
        "act_d = sigmoid_d\n",
        "\n",
        "model = NeuralNetwork(x_train.shape[-1])\n",
        "model.add_layer(FullyConnectedLayer(100))\n",
        "model.add_layer(ActivationLayer(act, act_d))\n",
        "model.add_layer(FullyConnectedLayer(100))\n",
        "model.add_layer(ActivationLayer(act, act_d))\n",
        "model.add_layer(FullyConnectedLayer(10))\n",
        "model.add_layer(ActivationLayer(act, act_d))\n",
        "model.set_loss_function(mse, mse_d)\n",
        "\n",
        "def calc_loss(model, X, Y, batch_size, loss_func):\n",
        "    error = 0\n",
        "    num_batches = math.ceil(X.shape[0] / batch_size)\n",
        "    for i in range(0, num_batches, 1):\n",
        "        X_batch = X[batch_size*i : batch_size*(i+1)]\n",
        "        Y_batch = Y[batch_size*i : batch_size*(i+1)]\n",
        "        \n",
        "        output = X_batch\n",
        "        for layer in model.layers:\n",
        "            output = layer.forward_prop(output)\n",
        "\n",
        "        Y_batch = Y_batch.reshape(output.shape)\n",
        "        e = loss_func(output, Y_batch) \n",
        "        error += e * X_batch.shape[0]\n",
        "    return error\n",
        "\n",
        "# Error per sample will be same magnitude\n",
        "# Mean of 1 sample will be same as mean of many samples\n",
        "# So with batches, sum of errors will be a factor of batch_size smaller\n",
        "# Multiply final error by batch_size?\n",
        "#print(calc_loss(model, x_train, y_train, 1, mse))\n",
        "#print(calc_loss(model, x_train, y_train, 100, mse))\n",
        "\n",
        "np.random.seed(10)\n",
        "#model.train(x_train[:1000], y_train[0:1000], 0.1, 30, batch_size=1)\n",
        "model.train(x_train[:1000], y_train[0:1000], 0.1*32, 500, batch_size=32)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "n_y = 10\n",
        "predicts = model.predict(x_test[:n_y])\n",
        "for i in range(n_y):\n",
        "    print(\"Predict={:d}  True={:d}\".format(np.argmax(predicts[i]),np.argmax(y_test[i])))\n"
      ],
      "metadata": {
        "id": "6d7st2DGItW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Batch/minibatch updates, input shapes, softmax, crossentropy, refactor, check for others "
      ],
      "metadata": {
        "id": "FGUdZjzokrGZ"
      }
    }
  ]
}