{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnf7uFR8CPy+Zp8n37PJ3F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install numpy"
      ],
      "metadata": {
        "id": "SjaKhDA3Aeuy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "wWV9-2TgVg9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "import math"
      ],
      "metadata": {
        "id": "4yDsO-3fAdQG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Neural Network Class"
      ],
      "metadata": {
        "id": "_CfX12l3VjVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Currently only implements a linear network structure\n",
        "# input_shape does not include batch size\n",
        "# input_shape needs to be refactored.  Currently takes an int\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        input_shape = self.layers[-1].output_shape if len(self.layers) > 0 else self.input_shape \n",
        "        layer.build(input_shape=input_shape)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def set_loss_function(self, loss_func, loss_deriv):\n",
        "        self.loss_func = loss_func\n",
        "        self.loss_deriv = loss_deriv\n",
        "\n",
        "    # Lacks error checking on input dimensions\n",
        "    def train(self, X, Y, lr, epochs, batch_size=None):\n",
        "        num_samples = X.shape[0]\n",
        "        if batch_size is None or batch_size > num_samples:\n",
        "            batch_size = num_samples\n",
        "        num_batches = math.ceil(num_samples / batch_size)\n",
        "\n",
        "        # NOTE: this method is quick and dirty and will only work for a linear network\n",
        "        for epoch in range(epochs):\n",
        "            error = 0\n",
        "\n",
        "            # Should probably shuffle batches(or samples?)...\n",
        "            for i in range(0, num_batches, 1):\n",
        "                X_batch = X[batch_size*i : batch_size*(i+1)]\n",
        "                Y_batch = Y[batch_size*i : batch_size*(i+1)]\n",
        "                \n",
        "                output = X_batch\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_prop(output)\n",
        "\n",
        "                Y_batch = Y_batch.reshape(output.shape)\n",
        "\n",
        "                # Compute reported error(loss)\n",
        "                # In Keras, reported losses are the average of per sample losses in each batch\n",
        "                # Assumption: error function returns avg error of samples within batch\n",
        "                # Multiply by number of samples in batch, then later divide by total number of samples\n",
        "                # This accounts for variable batch size\n",
        "                error += self.loss_func(output, Y_batch) * X_batch.shape[0]\n",
        "\n",
        "                error_gradient = self.loss_deriv(output, Y_batch)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error_gradient = layer.backprop(error_gradient)\n",
        "\n",
        "                # Update using the computed weight gradients\n",
        "                for layer in self.layers:\n",
        "                    layer.update(lr)\n",
        "\n",
        "            # Divide total error by number of samples for per-sample mean error\n",
        "            error /= len(X)\n",
        "            \n",
        "            print(\"Epoch {:d}: {:f}\".format(epoch, error))\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = X\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward_prop(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XKt02umYAhxt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Layer Class"
      ],
      "metadata": {
        "id": "0ysiZQ1mI1bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(ABC):\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "\n",
        "    # Used to set expected input, output dimensions once adjacent layers are known,\n",
        "    # as well as construct weight matrices\n",
        "    @abstractmethod\n",
        "    def build(self, input_shape=None, output_shape=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward_prop(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backprop(self, error, lr):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def update(self, lr):\n",
        "        return\n",
        "\n"
      ],
      "metadata": {
        "id": "vWIAYxsZZH1g"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully Connected(Dense) Layer"
      ],
      "metadata": {
        "id": "dkG0AwQxJGEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# aka Dense Layer\n",
        "class FullyConnectedLayer(Layer):\n",
        "    def __init__(self, neurons, input_shape=None, weight_range=(-0.5,0.5)):\n",
        "        super().__init__()\n",
        "        self.neurons = neurons\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = neurons\n",
        "        self.weight_range = weight_range\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.grad_weights = None\n",
        "        self.bias_weights = None\n",
        "        self.num_samples_used = 0\n",
        "\n",
        "    def build(self, input_shape=None, output_shape=None):\n",
        "        # Output shape is equal to neurons for standard dense layer\n",
        "        self.input_shape = input_shape or self.input_shape\n",
        "        self.init_weights()\n",
        "\n",
        "    # Weight initialization\n",
        "    #   See https://www.analyticsvidhya.com/blog/2021/05/how-to-initialize-weights-in-neural-networks/\n",
        "    #   -Small magnitude is recommended\n",
        "    #   -Heuristics are good\n",
        "    #   -Just randomizing with a range of 1\n",
        "    def init_weights(self):\n",
        "        min_w = self.weight_range[0]\n",
        "        max_w = self.weight_range[1]\n",
        "        self.weights = np.random.uniform(min_w, max_w, (self.input_shape, self.neurons))\n",
        "        self.bias = np.zeros((1, self.neurons))\n",
        "        # Matrices for summed weight gradients during backprop\n",
        "        # Used to store gradients for post-backprop GD update\n",
        "        self.grad_weights = np.zeros(self.weights.shape)\n",
        "        self.grad_bias = np.zeros(self.bias.shape)\n",
        "        # Stores number of samples adding to current gradient sum matrices\n",
        "        self.num_samples_used = 0  \n",
        "\n",
        "    def forward_prop(self, inputs):\n",
        "        # Track amount of samples included in this batch so far\n",
        "        # Required for averaging sum of sample gradients in update\n",
        "        self.num_samples_used += inputs.shape[0]\n",
        "\n",
        "        # Y = XW + B, where\n",
        "        #   X is vector of inputs\n",
        "        #   W is matrix of weights\n",
        "        #   B is column of biases\n",
        "        outputs = np.matmul(inputs, self.weights) + self.bias\n",
        "        self.inputs = inputs\n",
        "        return outputs\n",
        "\n",
        "    # dE_dY is of shape (1, neurons)\n",
        "    def backprop(self, dE_dY):\n",
        "        # Compute gradient\n",
        "        # Recall Y = XW + B, where\n",
        "        #   X is vector of inputs\n",
        "        #   W is matrix of weights\n",
        "        #   B is column of biases\n",
        "        # So, for a given weight wij(neuron i, weight from input j)\n",
        "        #   yi = xj*wij + bi\n",
        "        #   dE/dwij = dE/dyi * dyi/dwij             = dE/dyi * xj\n",
        "        #   dE/dbi  = dE/dyi * dyi/dbi = dE/dyi * 1 = dE/dyi\n",
        "        # (inputs, outputs) = (inputs, 1) . (1, outputs)\n",
        "        transpose = self.inputs.swapaxes(-1,-2)\n",
        "        dE_dW = np.matmul(transpose, dE_dY)\n",
        "        dE_dB = dE_dY  \n",
        "\n",
        "        # For each layer, have matrix of weight/bias derivatives matching weight dimensions\n",
        "        # Add onto it for each sample, then divide by batch size for avg deriv\n",
        "        self.grad_weights += np.sum(dE_dW, axis=0)\n",
        "        self.grad_bias += np.sum(dE_dB, axis=0)\n",
        "\n",
        "        # Pass along error gradient(dE_dX)\n",
        "        # Y(output) of previous layer is this layer's X(input)\n",
        "        #   dE/dxj = dE/dyi * dyi/dxj               = dE/dyi * wij\n",
        "        # (1, inputs) = (1, outputs) . (outputs, inputs)\n",
        "        dE_dX = np.dot(dE_dY, self.weights.T)\n",
        "        return dE_dX\n",
        "\n",
        "    def update(self, lr):\n",
        "        # Average summed weight gradients by dividing by number of samples in batch\n",
        "        self.grad_weights /= self.num_samples_used\n",
        "        self.grad_bias /= self.num_samples_used\n",
        "\n",
        "        # Update via gradient descent\n",
        "        self.weights  = self.weights - (lr * self.grad_weights)\n",
        "        self.bias = self.bias - (lr * self.grad_bias)\n",
        "\n",
        "        # Reset gradient sums, batch size count for next batch\n",
        "        self.grad_weights = np.zeros(self.weights.shape)\n",
        "        self.grad_bias = np.zeros(self.bias.shape)\n",
        "        self.num_samples_used = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "c1QL5cZh7iCC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Layer"
      ],
      "metadata": {
        "id": "zbxpdl-9It14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationLayer(Layer):\n",
        "    def __init__(self,activation_func, derivative_func):\n",
        "        super().__init__()\n",
        "        self.activation = activation_func\n",
        "        self.derivative = derivative_func\n",
        "\n",
        "    def build(self, input_shape=None, output_shape=None):\n",
        "        if input_shape is not None:\n",
        "            self.input_shape = input_shape\n",
        "        self.output_shape = self.input_shape\n",
        "\n",
        "    def forward_prop(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        outputs = self.activation(inputs)\n",
        "        return outputs\n",
        "\n",
        "    # dE_dY = (1, outputs)\n",
        "    # derivative(inputs) is another vector of (1, inputs)\n",
        "    # |inputs| = |outputs| since just applying function to each  \n",
        "    def backprop(self, dE_dY, lr):\n",
        "        dY_dX = self.derivative(self.inputs)\n",
        "        dE_dX = dE_dY * dY_dX\n",
        "        return dE_dX\n",
        "\n",
        "    def update(self, lr):\n",
        "        return\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "Rnybvh2O7QBR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activations"
      ],
      "metadata": {
        "id": "RmAu5PUZInBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def relu_d(x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_d(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "    # Equivalent to?\n",
        "    #return np.exp(-x) / np.power(1 + np.exp(-x), 2)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_d(x):\n",
        "    return 1 - np.power(np.tanh(x),2)\n",
        "\n",
        "# Temperature controls \"confidence\"\n",
        "# AKA low temperature(<1) means high values will be counted more strongly, small values even smaller\n",
        "# High temperature(>1) means everything is more similar\n",
        "def softmax(X, temp=1):\n",
        "    E = np.exp(X)\n",
        "    sum = np.sum(np.exp(X))\n",
        "    return E / sum\n",
        "\n",
        "def softmax_d(X):\n",
        "    return softmax(X)\n",
        "    #raise NotImplementedError"
      ],
      "metadata": {
        "id": "yYrY1vBV1F5-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Functions"
      ],
      "metadata": {
        "id": "SGb6nvQbJb91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "class LossFunction(ABC):\n",
        "    def __init__(self):\n",
        "        self.func = None\n",
        "        self.deriv = None  \n",
        "\n",
        "# Sum of squared errors for all samples divided by num samples\n",
        "# Will this work for batch?\n",
        "# https://stackoverflow.com/questions/55936214/correct-way-to-calculate-mse-for-autoencoders-with-batch-training\n",
        "def mse(y_pred, y_true):\n",
        "    #return np.mean(np.power(y_true - y_pred, 2))\n",
        "    return np.mean(np.mean(np.power(y_true - y_pred, 2), axis=1))\n",
        "\n",
        "def mse_d(y_pred, y_true):\n",
        "    return -2 * (y_true - y_pred) / y_true.size;\n",
        "\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def cross_entropy_d(y_pred, y_true):\n",
        "    raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "LcTB1lyPJeHF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Test"
      ],
      "metadata": {
        "id": "SxV8AWWWwLgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "\n",
        "#x_train = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "#y_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "#print(x_train.shape)\n",
        "#print(y_train.shape)\n",
        "\n",
        "model = NeuralNetwork(x_train.shape[-1]) #(x_train.shape[1:])\n",
        "model.add_layer(FullyConnectedLayer(3))\n",
        "model.add_layer(ActivationLayer(tanh, tanh_d))\n",
        "model.add_layer(FullyConnectedLayer(1))\n",
        "model.add_layer(ActivationLayer(tanh, tanh_d))\n",
        "model.set_loss_function(mse, mse_d)\n",
        "\n",
        "model.train(x_train, y_train, 0.1, 1000, batch_size=1)\n",
        "\n",
        "pred = model.predict(x_train)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "fCBUqLy9ByL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd25f52-3d94-4bf7-cbc6-2b18d6ec524c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 0.416257\n",
            "Epoch 1: 0.312337\n",
            "Epoch 2: 0.293873\n",
            "Epoch 3: 0.288785\n",
            "Epoch 4: 0.286772\n",
            "Epoch 5: 0.285680\n",
            "Epoch 6: 0.284918\n",
            "Epoch 7: 0.284286\n",
            "Epoch 8: 0.283708\n",
            "Epoch 9: 0.283148\n",
            "Epoch 10: 0.282590\n",
            "Epoch 11: 0.282023\n",
            "Epoch 12: 0.281442\n",
            "Epoch 13: 0.280841\n",
            "Epoch 14: 0.280219\n",
            "Epoch 15: 0.279572\n",
            "Epoch 16: 0.278899\n",
            "Epoch 17: 0.278198\n",
            "Epoch 18: 0.277466\n",
            "Epoch 19: 0.276702\n",
            "Epoch 20: 0.275904\n",
            "Epoch 21: 0.275071\n",
            "Epoch 22: 0.274202\n",
            "Epoch 23: 0.273296\n",
            "Epoch 24: 0.272350\n",
            "Epoch 25: 0.271365\n",
            "Epoch 26: 0.270339\n",
            "Epoch 27: 0.269272\n",
            "Epoch 28: 0.268163\n",
            "Epoch 29: 0.267012\n",
            "Epoch 30: 0.265819\n",
            "Epoch 31: 0.264585\n",
            "Epoch 32: 0.263310\n",
            "Epoch 33: 0.261994\n",
            "Epoch 34: 0.260640\n",
            "Epoch 35: 0.259248\n",
            "Epoch 36: 0.257820\n",
            "Epoch 37: 0.256359\n",
            "Epoch 38: 0.254867\n",
            "Epoch 39: 0.253347\n",
            "Epoch 40: 0.251802\n",
            "Epoch 41: 0.250237\n",
            "Epoch 42: 0.248654\n",
            "Epoch 43: 0.247059\n",
            "Epoch 44: 0.245456\n",
            "Epoch 45: 0.243848\n",
            "Epoch 46: 0.242242\n",
            "Epoch 47: 0.240640\n",
            "Epoch 48: 0.239049\n",
            "Epoch 49: 0.237473\n",
            "Epoch 50: 0.235915\n",
            "Epoch 51: 0.234380\n",
            "Epoch 52: 0.232872\n",
            "Epoch 53: 0.231393\n",
            "Epoch 54: 0.229947\n",
            "Epoch 55: 0.228536\n",
            "Epoch 56: 0.227161\n",
            "Epoch 57: 0.225825\n",
            "Epoch 58: 0.224528\n",
            "Epoch 59: 0.223270\n",
            "Epoch 60: 0.222052\n",
            "Epoch 61: 0.220873\n",
            "Epoch 62: 0.219733\n",
            "Epoch 63: 0.218631\n",
            "Epoch 64: 0.217565\n",
            "Epoch 65: 0.216535\n",
            "Epoch 66: 0.215538\n",
            "Epoch 67: 0.214573\n",
            "Epoch 68: 0.213639\n",
            "Epoch 69: 0.212732\n",
            "Epoch 70: 0.211852\n",
            "Epoch 71: 0.210996\n",
            "Epoch 72: 0.210162\n",
            "Epoch 73: 0.209349\n",
            "Epoch 74: 0.208554\n",
            "Epoch 75: 0.207775\n",
            "Epoch 76: 0.207011\n",
            "Epoch 77: 0.206260\n",
            "Epoch 78: 0.205519\n",
            "Epoch 79: 0.204788\n",
            "Epoch 80: 0.204063\n",
            "Epoch 81: 0.203344\n",
            "Epoch 82: 0.202630\n",
            "Epoch 83: 0.201917\n",
            "Epoch 84: 0.201205\n",
            "Epoch 85: 0.200492\n",
            "Epoch 86: 0.199776\n",
            "Epoch 87: 0.199056\n",
            "Epoch 88: 0.198331\n",
            "Epoch 89: 0.197598\n",
            "Epoch 90: 0.196857\n",
            "Epoch 91: 0.196106\n",
            "Epoch 92: 0.195344\n",
            "Epoch 93: 0.194569\n",
            "Epoch 94: 0.193780\n",
            "Epoch 95: 0.192975\n",
            "Epoch 96: 0.192154\n",
            "Epoch 97: 0.191314\n",
            "Epoch 98: 0.190455\n",
            "Epoch 99: 0.189574\n",
            "Epoch 100: 0.188671\n",
            "Epoch 101: 0.187745\n",
            "Epoch 102: 0.186793\n",
            "Epoch 103: 0.185815\n",
            "Epoch 104: 0.184809\n",
            "Epoch 105: 0.183774\n",
            "Epoch 106: 0.182707\n",
            "Epoch 107: 0.181609\n",
            "Epoch 108: 0.180476\n",
            "Epoch 109: 0.179307\n",
            "Epoch 110: 0.178101\n",
            "Epoch 111: 0.176855\n",
            "Epoch 112: 0.175568\n",
            "Epoch 113: 0.174237\n",
            "Epoch 114: 0.172861\n",
            "Epoch 115: 0.171437\n",
            "Epoch 116: 0.169962\n",
            "Epoch 117: 0.168435\n",
            "Epoch 118: 0.166851\n",
            "Epoch 119: 0.165210\n",
            "Epoch 120: 0.163507\n",
            "Epoch 121: 0.161740\n",
            "Epoch 122: 0.159905\n",
            "Epoch 123: 0.158001\n",
            "Epoch 124: 0.156023\n",
            "Epoch 125: 0.153970\n",
            "Epoch 126: 0.151837\n",
            "Epoch 127: 0.149623\n",
            "Epoch 128: 0.147325\n",
            "Epoch 129: 0.144941\n",
            "Epoch 130: 0.142469\n",
            "Epoch 131: 0.139908\n",
            "Epoch 132: 0.137256\n",
            "Epoch 133: 0.134513\n",
            "Epoch 134: 0.131679\n",
            "Epoch 135: 0.128754\n",
            "Epoch 136: 0.125741\n",
            "Epoch 137: 0.122641\n",
            "Epoch 138: 0.119458\n",
            "Epoch 139: 0.116196\n",
            "Epoch 140: 0.112859\n",
            "Epoch 141: 0.109456\n",
            "Epoch 142: 0.105992\n",
            "Epoch 143: 0.102478\n",
            "Epoch 144: 0.098923\n",
            "Epoch 145: 0.095338\n",
            "Epoch 146: 0.091736\n",
            "Epoch 147: 0.088129\n",
            "Epoch 148: 0.084532\n",
            "Epoch 149: 0.080959\n",
            "Epoch 150: 0.077424\n",
            "Epoch 151: 0.073943\n",
            "Epoch 152: 0.070528\n",
            "Epoch 153: 0.067194\n",
            "Epoch 154: 0.063951\n",
            "Epoch 155: 0.060812\n",
            "Epoch 156: 0.057784\n",
            "Epoch 157: 0.054876\n",
            "Epoch 158: 0.052093\n",
            "Epoch 159: 0.049438\n",
            "Epoch 160: 0.046913\n",
            "Epoch 161: 0.044520\n",
            "Epoch 162: 0.042255\n",
            "Epoch 163: 0.040119\n",
            "Epoch 164: 0.038105\n",
            "Epoch 165: 0.036212\n",
            "Epoch 166: 0.034433\n",
            "Epoch 167: 0.032763\n",
            "Epoch 168: 0.031197\n",
            "Epoch 169: 0.029729\n",
            "Epoch 170: 0.028353\n",
            "Epoch 171: 0.027063\n",
            "Epoch 172: 0.025854\n",
            "Epoch 173: 0.024721\n",
            "Epoch 174: 0.023658\n",
            "Epoch 175: 0.022661\n",
            "Epoch 176: 0.021725\n",
            "Epoch 177: 0.020846\n",
            "Epoch 178: 0.020020\n",
            "Epoch 179: 0.019242\n",
            "Epoch 180: 0.018510\n",
            "Epoch 181: 0.017821\n",
            "Epoch 182: 0.017171\n",
            "Epoch 183: 0.016557\n",
            "Epoch 184: 0.015977\n",
            "Epoch 185: 0.015429\n",
            "Epoch 186: 0.014911\n",
            "Epoch 187: 0.014420\n",
            "Epoch 188: 0.013955\n",
            "Epoch 189: 0.013514\n",
            "Epoch 190: 0.013094\n",
            "Epoch 191: 0.012696\n",
            "Epoch 192: 0.012318\n",
            "Epoch 193: 0.011957\n",
            "Epoch 194: 0.011614\n",
            "Epoch 195: 0.011287\n",
            "Epoch 196: 0.010975\n",
            "Epoch 197: 0.010677\n",
            "Epoch 198: 0.010393\n",
            "Epoch 199: 0.010121\n",
            "Epoch 200: 0.009860\n",
            "Epoch 201: 0.009611\n",
            "Epoch 202: 0.009372\n",
            "Epoch 203: 0.009144\n",
            "Epoch 204: 0.008924\n",
            "Epoch 205: 0.008713\n",
            "Epoch 206: 0.008511\n",
            "Epoch 207: 0.008317\n",
            "Epoch 208: 0.008130\n",
            "Epoch 209: 0.007950\n",
            "Epoch 210: 0.007777\n",
            "Epoch 211: 0.007610\n",
            "Epoch 212: 0.007449\n",
            "Epoch 213: 0.007294\n",
            "Epoch 214: 0.007145\n",
            "Epoch 215: 0.007001\n",
            "Epoch 216: 0.006861\n",
            "Epoch 217: 0.006727\n",
            "Epoch 218: 0.006597\n",
            "Epoch 219: 0.006471\n",
            "Epoch 220: 0.006349\n",
            "Epoch 221: 0.006232\n",
            "Epoch 222: 0.006118\n",
            "Epoch 223: 0.006007\n",
            "Epoch 224: 0.005901\n",
            "Epoch 225: 0.005797\n",
            "Epoch 226: 0.005696\n",
            "Epoch 227: 0.005599\n",
            "Epoch 228: 0.005504\n",
            "Epoch 229: 0.005412\n",
            "Epoch 230: 0.005323\n",
            "Epoch 231: 0.005237\n",
            "Epoch 232: 0.005152\n",
            "Epoch 233: 0.005071\n",
            "Epoch 234: 0.004991\n",
            "Epoch 235: 0.004914\n",
            "Epoch 236: 0.004838\n",
            "Epoch 237: 0.004765\n",
            "Epoch 238: 0.004694\n",
            "Epoch 239: 0.004624\n",
            "Epoch 240: 0.004556\n",
            "Epoch 241: 0.004490\n",
            "Epoch 242: 0.004426\n",
            "Epoch 243: 0.004363\n",
            "Epoch 244: 0.004302\n",
            "Epoch 245: 0.004243\n",
            "Epoch 246: 0.004184\n",
            "Epoch 247: 0.004128\n",
            "Epoch 248: 0.004072\n",
            "Epoch 249: 0.004018\n",
            "Epoch 250: 0.003965\n",
            "Epoch 251: 0.003914\n",
            "Epoch 252: 0.003863\n",
            "Epoch 253: 0.003814\n",
            "Epoch 254: 0.003766\n",
            "Epoch 255: 0.003719\n",
            "Epoch 256: 0.003672\n",
            "Epoch 257: 0.003627\n",
            "Epoch 258: 0.003583\n",
            "Epoch 259: 0.003540\n",
            "Epoch 260: 0.003498\n",
            "Epoch 261: 0.003457\n",
            "Epoch 262: 0.003416\n",
            "Epoch 263: 0.003376\n",
            "Epoch 264: 0.003338\n",
            "Epoch 265: 0.003300\n",
            "Epoch 266: 0.003262\n",
            "Epoch 267: 0.003226\n",
            "Epoch 268: 0.003190\n",
            "Epoch 269: 0.003155\n",
            "Epoch 270: 0.003120\n",
            "Epoch 271: 0.003087\n",
            "Epoch 272: 0.003054\n",
            "Epoch 273: 0.003021\n",
            "Epoch 274: 0.002989\n",
            "Epoch 275: 0.002958\n",
            "Epoch 276: 0.002927\n",
            "Epoch 277: 0.002897\n",
            "Epoch 278: 0.002868\n",
            "Epoch 279: 0.002839\n",
            "Epoch 280: 0.002810\n",
            "Epoch 281: 0.002782\n",
            "Epoch 282: 0.002755\n",
            "Epoch 283: 0.002728\n",
            "Epoch 284: 0.002701\n",
            "Epoch 285: 0.002675\n",
            "Epoch 286: 0.002650\n",
            "Epoch 287: 0.002624\n",
            "Epoch 288: 0.002600\n",
            "Epoch 289: 0.002575\n",
            "Epoch 290: 0.002551\n",
            "Epoch 291: 0.002528\n",
            "Epoch 292: 0.002505\n",
            "Epoch 293: 0.002482\n",
            "Epoch 294: 0.002459\n",
            "Epoch 295: 0.002437\n",
            "Epoch 296: 0.002416\n",
            "Epoch 297: 0.002394\n",
            "Epoch 298: 0.002373\n",
            "Epoch 299: 0.002353\n",
            "Epoch 300: 0.002332\n",
            "Epoch 301: 0.002312\n",
            "Epoch 302: 0.002293\n",
            "Epoch 303: 0.002273\n",
            "Epoch 304: 0.002254\n",
            "Epoch 305: 0.002235\n",
            "Epoch 306: 0.002217\n",
            "Epoch 307: 0.002198\n",
            "Epoch 308: 0.002180\n",
            "Epoch 309: 0.002163\n",
            "Epoch 310: 0.002145\n",
            "Epoch 311: 0.002128\n",
            "Epoch 312: 0.002111\n",
            "Epoch 313: 0.002094\n",
            "Epoch 314: 0.002078\n",
            "Epoch 315: 0.002062\n",
            "Epoch 316: 0.002045\n",
            "Epoch 317: 0.002030\n",
            "Epoch 318: 0.002014\n",
            "Epoch 319: 0.001999\n",
            "Epoch 320: 0.001984\n",
            "Epoch 321: 0.001969\n",
            "Epoch 322: 0.001954\n",
            "Epoch 323: 0.001939\n",
            "Epoch 324: 0.001925\n",
            "Epoch 325: 0.001911\n",
            "Epoch 326: 0.001897\n",
            "Epoch 327: 0.001883\n",
            "Epoch 328: 0.001869\n",
            "Epoch 329: 0.001856\n",
            "Epoch 330: 0.001843\n",
            "Epoch 331: 0.001830\n",
            "Epoch 332: 0.001817\n",
            "Epoch 333: 0.001804\n",
            "Epoch 334: 0.001791\n",
            "Epoch 335: 0.001779\n",
            "Epoch 336: 0.001767\n",
            "Epoch 337: 0.001754\n",
            "Epoch 338: 0.001742\n",
            "Epoch 339: 0.001731\n",
            "Epoch 340: 0.001719\n",
            "Epoch 341: 0.001707\n",
            "Epoch 342: 0.001696\n",
            "Epoch 343: 0.001685\n",
            "Epoch 344: 0.001674\n",
            "Epoch 345: 0.001663\n",
            "Epoch 346: 0.001652\n",
            "Epoch 347: 0.001641\n",
            "Epoch 348: 0.001630\n",
            "Epoch 349: 0.001620\n",
            "Epoch 350: 0.001610\n",
            "Epoch 351: 0.001599\n",
            "Epoch 352: 0.001589\n",
            "Epoch 353: 0.001579\n",
            "Epoch 354: 0.001569\n",
            "Epoch 355: 0.001559\n",
            "Epoch 356: 0.001550\n",
            "Epoch 357: 0.001540\n",
            "Epoch 358: 0.001531\n",
            "Epoch 359: 0.001521\n",
            "Epoch 360: 0.001512\n",
            "Epoch 361: 0.001503\n",
            "Epoch 362: 0.001494\n",
            "Epoch 363: 0.001485\n",
            "Epoch 364: 0.001476\n",
            "Epoch 365: 0.001467\n",
            "Epoch 366: 0.001459\n",
            "Epoch 367: 0.001450\n",
            "Epoch 368: 0.001442\n",
            "Epoch 369: 0.001433\n",
            "Epoch 370: 0.001425\n",
            "Epoch 371: 0.001417\n",
            "Epoch 372: 0.001409\n",
            "Epoch 373: 0.001401\n",
            "Epoch 374: 0.001393\n",
            "Epoch 375: 0.001385\n",
            "Epoch 376: 0.001377\n",
            "Epoch 377: 0.001369\n",
            "Epoch 378: 0.001362\n",
            "Epoch 379: 0.001354\n",
            "Epoch 380: 0.001347\n",
            "Epoch 381: 0.001339\n",
            "Epoch 382: 0.001332\n",
            "Epoch 383: 0.001325\n",
            "Epoch 384: 0.001317\n",
            "Epoch 385: 0.001310\n",
            "Epoch 386: 0.001303\n",
            "Epoch 387: 0.001296\n",
            "Epoch 388: 0.001289\n",
            "Epoch 389: 0.001283\n",
            "Epoch 390: 0.001276\n",
            "Epoch 391: 0.001269\n",
            "Epoch 392: 0.001262\n",
            "Epoch 393: 0.001256\n",
            "Epoch 394: 0.001249\n",
            "Epoch 395: 0.001243\n",
            "Epoch 396: 0.001236\n",
            "Epoch 397: 0.001230\n",
            "Epoch 398: 0.001224\n",
            "Epoch 399: 0.001218\n",
            "Epoch 400: 0.001211\n",
            "Epoch 401: 0.001205\n",
            "Epoch 402: 0.001199\n",
            "Epoch 403: 0.001193\n",
            "Epoch 404: 0.001187\n",
            "Epoch 405: 0.001181\n",
            "Epoch 406: 0.001176\n",
            "Epoch 407: 0.001170\n",
            "Epoch 408: 0.001164\n",
            "Epoch 409: 0.001158\n",
            "Epoch 410: 0.001153\n",
            "Epoch 411: 0.001147\n",
            "Epoch 412: 0.001142\n",
            "Epoch 413: 0.001136\n",
            "Epoch 414: 0.001131\n",
            "Epoch 415: 0.001125\n",
            "Epoch 416: 0.001120\n",
            "Epoch 417: 0.001115\n",
            "Epoch 418: 0.001110\n",
            "Epoch 419: 0.001104\n",
            "Epoch 420: 0.001099\n",
            "Epoch 421: 0.001094\n",
            "Epoch 422: 0.001089\n",
            "Epoch 423: 0.001084\n",
            "Epoch 424: 0.001079\n",
            "Epoch 425: 0.001074\n",
            "Epoch 426: 0.001069\n",
            "Epoch 427: 0.001064\n",
            "Epoch 428: 0.001059\n",
            "Epoch 429: 0.001055\n",
            "Epoch 430: 0.001050\n",
            "Epoch 431: 0.001045\n",
            "Epoch 432: 0.001041\n",
            "Epoch 433: 0.001036\n",
            "Epoch 434: 0.001031\n",
            "Epoch 435: 0.001027\n",
            "Epoch 436: 0.001022\n",
            "Epoch 437: 0.001018\n",
            "Epoch 438: 0.001013\n",
            "Epoch 439: 0.001009\n",
            "Epoch 440: 0.001005\n",
            "Epoch 441: 0.001000\n",
            "Epoch 442: 0.000996\n",
            "Epoch 443: 0.000992\n",
            "Epoch 444: 0.000987\n",
            "Epoch 445: 0.000983\n",
            "Epoch 446: 0.000979\n",
            "Epoch 447: 0.000975\n",
            "Epoch 448: 0.000971\n",
            "Epoch 449: 0.000967\n",
            "Epoch 450: 0.000963\n",
            "Epoch 451: 0.000959\n",
            "Epoch 452: 0.000955\n",
            "Epoch 453: 0.000951\n",
            "Epoch 454: 0.000947\n",
            "Epoch 455: 0.000943\n",
            "Epoch 456: 0.000939\n",
            "Epoch 457: 0.000935\n",
            "Epoch 458: 0.000932\n",
            "Epoch 459: 0.000928\n",
            "Epoch 460: 0.000924\n",
            "Epoch 461: 0.000920\n",
            "Epoch 462: 0.000917\n",
            "Epoch 463: 0.000913\n",
            "Epoch 464: 0.000909\n",
            "Epoch 465: 0.000906\n",
            "Epoch 466: 0.000902\n",
            "Epoch 467: 0.000899\n",
            "Epoch 468: 0.000895\n",
            "Epoch 469: 0.000891\n",
            "Epoch 470: 0.000888\n",
            "Epoch 471: 0.000885\n",
            "Epoch 472: 0.000881\n",
            "Epoch 473: 0.000878\n",
            "Epoch 474: 0.000874\n",
            "Epoch 475: 0.000871\n",
            "Epoch 476: 0.000868\n",
            "Epoch 477: 0.000864\n",
            "Epoch 478: 0.000861\n",
            "Epoch 479: 0.000858\n",
            "Epoch 480: 0.000855\n",
            "Epoch 481: 0.000851\n",
            "Epoch 482: 0.000848\n",
            "Epoch 483: 0.000845\n",
            "Epoch 484: 0.000842\n",
            "Epoch 485: 0.000839\n",
            "Epoch 486: 0.000836\n",
            "Epoch 487: 0.000832\n",
            "Epoch 488: 0.000829\n",
            "Epoch 489: 0.000826\n",
            "Epoch 490: 0.000823\n",
            "Epoch 491: 0.000820\n",
            "Epoch 492: 0.000817\n",
            "Epoch 493: 0.000814\n",
            "Epoch 494: 0.000811\n",
            "Epoch 495: 0.000808\n",
            "Epoch 496: 0.000806\n",
            "Epoch 497: 0.000803\n",
            "Epoch 498: 0.000800\n",
            "Epoch 499: 0.000797\n",
            "Epoch 500: 0.000794\n",
            "Epoch 501: 0.000791\n",
            "Epoch 502: 0.000788\n",
            "Epoch 503: 0.000786\n",
            "Epoch 504: 0.000783\n",
            "Epoch 505: 0.000780\n",
            "Epoch 506: 0.000777\n",
            "Epoch 507: 0.000775\n",
            "Epoch 508: 0.000772\n",
            "Epoch 509: 0.000769\n",
            "Epoch 510: 0.000767\n",
            "Epoch 511: 0.000764\n",
            "Epoch 512: 0.000761\n",
            "Epoch 513: 0.000759\n",
            "Epoch 514: 0.000756\n",
            "Epoch 515: 0.000754\n",
            "Epoch 516: 0.000751\n",
            "Epoch 517: 0.000749\n",
            "Epoch 518: 0.000746\n",
            "Epoch 519: 0.000744\n",
            "Epoch 520: 0.000741\n",
            "Epoch 521: 0.000739\n",
            "Epoch 522: 0.000736\n",
            "Epoch 523: 0.000734\n",
            "Epoch 524: 0.000731\n",
            "Epoch 525: 0.000729\n",
            "Epoch 526: 0.000726\n",
            "Epoch 527: 0.000724\n",
            "Epoch 528: 0.000722\n",
            "Epoch 529: 0.000719\n",
            "Epoch 530: 0.000717\n",
            "Epoch 531: 0.000715\n",
            "Epoch 532: 0.000712\n",
            "Epoch 533: 0.000710\n",
            "Epoch 534: 0.000708\n",
            "Epoch 535: 0.000705\n",
            "Epoch 536: 0.000703\n",
            "Epoch 537: 0.000701\n",
            "Epoch 538: 0.000699\n",
            "Epoch 539: 0.000696\n",
            "Epoch 540: 0.000694\n",
            "Epoch 541: 0.000692\n",
            "Epoch 542: 0.000690\n",
            "Epoch 543: 0.000688\n",
            "Epoch 544: 0.000686\n",
            "Epoch 545: 0.000683\n",
            "Epoch 546: 0.000681\n",
            "Epoch 547: 0.000679\n",
            "Epoch 548: 0.000677\n",
            "Epoch 549: 0.000675\n",
            "Epoch 550: 0.000673\n",
            "Epoch 551: 0.000671\n",
            "Epoch 552: 0.000669\n",
            "Epoch 553: 0.000667\n",
            "Epoch 554: 0.000665\n",
            "Epoch 555: 0.000663\n",
            "Epoch 556: 0.000661\n",
            "Epoch 557: 0.000659\n",
            "Epoch 558: 0.000657\n",
            "Epoch 559: 0.000655\n",
            "Epoch 560: 0.000653\n",
            "Epoch 561: 0.000651\n",
            "Epoch 562: 0.000649\n",
            "Epoch 563: 0.000647\n",
            "Epoch 564: 0.000645\n",
            "Epoch 565: 0.000643\n",
            "Epoch 566: 0.000641\n",
            "Epoch 567: 0.000639\n",
            "Epoch 568: 0.000637\n",
            "Epoch 569: 0.000635\n",
            "Epoch 570: 0.000633\n",
            "Epoch 571: 0.000632\n",
            "Epoch 572: 0.000630\n",
            "Epoch 573: 0.000628\n",
            "Epoch 574: 0.000626\n",
            "Epoch 575: 0.000624\n",
            "Epoch 576: 0.000623\n",
            "Epoch 577: 0.000621\n",
            "Epoch 578: 0.000619\n",
            "Epoch 579: 0.000617\n",
            "Epoch 580: 0.000615\n",
            "Epoch 581: 0.000614\n",
            "Epoch 582: 0.000612\n",
            "Epoch 583: 0.000610\n",
            "Epoch 584: 0.000608\n",
            "Epoch 585: 0.000607\n",
            "Epoch 586: 0.000605\n",
            "Epoch 587: 0.000603\n",
            "Epoch 588: 0.000602\n",
            "Epoch 589: 0.000600\n",
            "Epoch 590: 0.000598\n",
            "Epoch 591: 0.000597\n",
            "Epoch 592: 0.000595\n",
            "Epoch 593: 0.000593\n",
            "Epoch 594: 0.000592\n",
            "Epoch 595: 0.000590\n",
            "Epoch 596: 0.000588\n",
            "Epoch 597: 0.000587\n",
            "Epoch 598: 0.000585\n",
            "Epoch 599: 0.000583\n",
            "Epoch 600: 0.000582\n",
            "Epoch 601: 0.000580\n",
            "Epoch 602: 0.000579\n",
            "Epoch 603: 0.000577\n",
            "Epoch 604: 0.000576\n",
            "Epoch 605: 0.000574\n",
            "Epoch 606: 0.000572\n",
            "Epoch 607: 0.000571\n",
            "Epoch 608: 0.000569\n",
            "Epoch 609: 0.000568\n",
            "Epoch 610: 0.000566\n",
            "Epoch 611: 0.000565\n",
            "Epoch 612: 0.000563\n",
            "Epoch 613: 0.000562\n",
            "Epoch 614: 0.000560\n",
            "Epoch 615: 0.000559\n",
            "Epoch 616: 0.000557\n",
            "Epoch 617: 0.000556\n",
            "Epoch 618: 0.000555\n",
            "Epoch 619: 0.000553\n",
            "Epoch 620: 0.000552\n",
            "Epoch 621: 0.000550\n",
            "Epoch 622: 0.000549\n",
            "Epoch 623: 0.000547\n",
            "Epoch 624: 0.000546\n",
            "Epoch 625: 0.000545\n",
            "Epoch 626: 0.000543\n",
            "Epoch 627: 0.000542\n",
            "Epoch 628: 0.000540\n",
            "Epoch 629: 0.000539\n",
            "Epoch 630: 0.000538\n",
            "Epoch 631: 0.000536\n",
            "Epoch 632: 0.000535\n",
            "Epoch 633: 0.000533\n",
            "Epoch 634: 0.000532\n",
            "Epoch 635: 0.000531\n",
            "Epoch 636: 0.000529\n",
            "Epoch 637: 0.000528\n",
            "Epoch 638: 0.000527\n",
            "Epoch 639: 0.000525\n",
            "Epoch 640: 0.000524\n",
            "Epoch 641: 0.000523\n",
            "Epoch 642: 0.000522\n",
            "Epoch 643: 0.000520\n",
            "Epoch 644: 0.000519\n",
            "Epoch 645: 0.000518\n",
            "Epoch 646: 0.000516\n",
            "Epoch 647: 0.000515\n",
            "Epoch 648: 0.000514\n",
            "Epoch 649: 0.000513\n",
            "Epoch 650: 0.000511\n",
            "Epoch 651: 0.000510\n",
            "Epoch 652: 0.000509\n",
            "Epoch 653: 0.000508\n",
            "Epoch 654: 0.000506\n",
            "Epoch 655: 0.000505\n",
            "Epoch 656: 0.000504\n",
            "Epoch 657: 0.000503\n",
            "Epoch 658: 0.000502\n",
            "Epoch 659: 0.000500\n",
            "Epoch 660: 0.000499\n",
            "Epoch 661: 0.000498\n",
            "Epoch 662: 0.000497\n",
            "Epoch 663: 0.000496\n",
            "Epoch 664: 0.000494\n",
            "Epoch 665: 0.000493\n",
            "Epoch 666: 0.000492\n",
            "Epoch 667: 0.000491\n",
            "Epoch 668: 0.000490\n",
            "Epoch 669: 0.000489\n",
            "Epoch 670: 0.000487\n",
            "Epoch 671: 0.000486\n",
            "Epoch 672: 0.000485\n",
            "Epoch 673: 0.000484\n",
            "Epoch 674: 0.000483\n",
            "Epoch 675: 0.000482\n",
            "Epoch 676: 0.000481\n",
            "Epoch 677: 0.000480\n",
            "Epoch 678: 0.000478\n",
            "Epoch 679: 0.000477\n",
            "Epoch 680: 0.000476\n",
            "Epoch 681: 0.000475\n",
            "Epoch 682: 0.000474\n",
            "Epoch 683: 0.000473\n",
            "Epoch 684: 0.000472\n",
            "Epoch 685: 0.000471\n",
            "Epoch 686: 0.000470\n",
            "Epoch 687: 0.000469\n",
            "Epoch 688: 0.000468\n",
            "Epoch 689: 0.000467\n",
            "Epoch 690: 0.000465\n",
            "Epoch 691: 0.000464\n",
            "Epoch 692: 0.000463\n",
            "Epoch 693: 0.000462\n",
            "Epoch 694: 0.000461\n",
            "Epoch 695: 0.000460\n",
            "Epoch 696: 0.000459\n",
            "Epoch 697: 0.000458\n",
            "Epoch 698: 0.000457\n",
            "Epoch 699: 0.000456\n",
            "Epoch 700: 0.000455\n",
            "Epoch 701: 0.000454\n",
            "Epoch 702: 0.000453\n",
            "Epoch 703: 0.000452\n",
            "Epoch 704: 0.000451\n",
            "Epoch 705: 0.000450\n",
            "Epoch 706: 0.000449\n",
            "Epoch 707: 0.000448\n",
            "Epoch 708: 0.000447\n",
            "Epoch 709: 0.000446\n",
            "Epoch 710: 0.000445\n",
            "Epoch 711: 0.000444\n",
            "Epoch 712: 0.000443\n",
            "Epoch 713: 0.000442\n",
            "Epoch 714: 0.000441\n",
            "Epoch 715: 0.000441\n",
            "Epoch 716: 0.000440\n",
            "Epoch 717: 0.000439\n",
            "Epoch 718: 0.000438\n",
            "Epoch 719: 0.000437\n",
            "Epoch 720: 0.000436\n",
            "Epoch 721: 0.000435\n",
            "Epoch 722: 0.000434\n",
            "Epoch 723: 0.000433\n",
            "Epoch 724: 0.000432\n",
            "Epoch 725: 0.000431\n",
            "Epoch 726: 0.000430\n",
            "Epoch 727: 0.000429\n",
            "Epoch 728: 0.000428\n",
            "Epoch 729: 0.000428\n",
            "Epoch 730: 0.000427\n",
            "Epoch 731: 0.000426\n",
            "Epoch 732: 0.000425\n",
            "Epoch 733: 0.000424\n",
            "Epoch 734: 0.000423\n",
            "Epoch 735: 0.000422\n",
            "Epoch 736: 0.000421\n",
            "Epoch 737: 0.000421\n",
            "Epoch 738: 0.000420\n",
            "Epoch 739: 0.000419\n",
            "Epoch 740: 0.000418\n",
            "Epoch 741: 0.000417\n",
            "Epoch 742: 0.000416\n",
            "Epoch 743: 0.000415\n",
            "Epoch 744: 0.000415\n",
            "Epoch 745: 0.000414\n",
            "Epoch 746: 0.000413\n",
            "Epoch 747: 0.000412\n",
            "Epoch 748: 0.000411\n",
            "Epoch 749: 0.000410\n",
            "Epoch 750: 0.000409\n",
            "Epoch 751: 0.000409\n",
            "Epoch 752: 0.000408\n",
            "Epoch 753: 0.000407\n",
            "Epoch 754: 0.000406\n",
            "Epoch 755: 0.000405\n",
            "Epoch 756: 0.000405\n",
            "Epoch 757: 0.000404\n",
            "Epoch 758: 0.000403\n",
            "Epoch 759: 0.000402\n",
            "Epoch 760: 0.000401\n",
            "Epoch 761: 0.000401\n",
            "Epoch 762: 0.000400\n",
            "Epoch 763: 0.000399\n",
            "Epoch 764: 0.000398\n",
            "Epoch 765: 0.000397\n",
            "Epoch 766: 0.000397\n",
            "Epoch 767: 0.000396\n",
            "Epoch 768: 0.000395\n",
            "Epoch 769: 0.000394\n",
            "Epoch 770: 0.000394\n",
            "Epoch 771: 0.000393\n",
            "Epoch 772: 0.000392\n",
            "Epoch 773: 0.000391\n",
            "Epoch 774: 0.000390\n",
            "Epoch 775: 0.000390\n",
            "Epoch 776: 0.000389\n",
            "Epoch 777: 0.000388\n",
            "Epoch 778: 0.000387\n",
            "Epoch 779: 0.000387\n",
            "Epoch 780: 0.000386\n",
            "Epoch 781: 0.000385\n",
            "Epoch 782: 0.000384\n",
            "Epoch 783: 0.000384\n",
            "Epoch 784: 0.000383\n",
            "Epoch 785: 0.000382\n",
            "Epoch 786: 0.000382\n",
            "Epoch 787: 0.000381\n",
            "Epoch 788: 0.000380\n",
            "Epoch 789: 0.000379\n",
            "Epoch 790: 0.000379\n",
            "Epoch 791: 0.000378\n",
            "Epoch 792: 0.000377\n",
            "Epoch 793: 0.000377\n",
            "Epoch 794: 0.000376\n",
            "Epoch 795: 0.000375\n",
            "Epoch 796: 0.000374\n",
            "Epoch 797: 0.000374\n",
            "Epoch 798: 0.000373\n",
            "Epoch 799: 0.000372\n",
            "Epoch 800: 0.000372\n",
            "Epoch 801: 0.000371\n",
            "Epoch 802: 0.000370\n",
            "Epoch 803: 0.000370\n",
            "Epoch 804: 0.000369\n",
            "Epoch 805: 0.000368\n",
            "Epoch 806: 0.000368\n",
            "Epoch 807: 0.000367\n",
            "Epoch 808: 0.000366\n",
            "Epoch 809: 0.000366\n",
            "Epoch 810: 0.000365\n",
            "Epoch 811: 0.000364\n",
            "Epoch 812: 0.000364\n",
            "Epoch 813: 0.000363\n",
            "Epoch 814: 0.000362\n",
            "Epoch 815: 0.000362\n",
            "Epoch 816: 0.000361\n",
            "Epoch 817: 0.000360\n",
            "Epoch 818: 0.000360\n",
            "Epoch 819: 0.000359\n",
            "Epoch 820: 0.000358\n",
            "Epoch 821: 0.000358\n",
            "Epoch 822: 0.000357\n",
            "Epoch 823: 0.000356\n",
            "Epoch 824: 0.000356\n",
            "Epoch 825: 0.000355\n",
            "Epoch 826: 0.000354\n",
            "Epoch 827: 0.000354\n",
            "Epoch 828: 0.000353\n",
            "Epoch 829: 0.000353\n",
            "Epoch 830: 0.000352\n",
            "Epoch 831: 0.000351\n",
            "Epoch 832: 0.000351\n",
            "Epoch 833: 0.000350\n",
            "Epoch 834: 0.000349\n",
            "Epoch 835: 0.000349\n",
            "Epoch 836: 0.000348\n",
            "Epoch 837: 0.000348\n",
            "Epoch 838: 0.000347\n",
            "Epoch 839: 0.000346\n",
            "Epoch 840: 0.000346\n",
            "Epoch 841: 0.000345\n",
            "Epoch 842: 0.000345\n",
            "Epoch 843: 0.000344\n",
            "Epoch 844: 0.000343\n",
            "Epoch 845: 0.000343\n",
            "Epoch 846: 0.000342\n",
            "Epoch 847: 0.000342\n",
            "Epoch 848: 0.000341\n",
            "Epoch 849: 0.000340\n",
            "Epoch 850: 0.000340\n",
            "Epoch 851: 0.000339\n",
            "Epoch 852: 0.000339\n",
            "Epoch 853: 0.000338\n",
            "Epoch 854: 0.000338\n",
            "Epoch 855: 0.000337\n",
            "Epoch 856: 0.000336\n",
            "Epoch 857: 0.000336\n",
            "Epoch 858: 0.000335\n",
            "Epoch 859: 0.000335\n",
            "Epoch 860: 0.000334\n",
            "Epoch 861: 0.000334\n",
            "Epoch 862: 0.000333\n",
            "Epoch 863: 0.000332\n",
            "Epoch 864: 0.000332\n",
            "Epoch 865: 0.000331\n",
            "Epoch 866: 0.000331\n",
            "Epoch 867: 0.000330\n",
            "Epoch 868: 0.000330\n",
            "Epoch 869: 0.000329\n",
            "Epoch 870: 0.000329\n",
            "Epoch 871: 0.000328\n",
            "Epoch 872: 0.000327\n",
            "Epoch 873: 0.000327\n",
            "Epoch 874: 0.000326\n",
            "Epoch 875: 0.000326\n",
            "Epoch 876: 0.000325\n",
            "Epoch 877: 0.000325\n",
            "Epoch 878: 0.000324\n",
            "Epoch 879: 0.000324\n",
            "Epoch 880: 0.000323\n",
            "Epoch 881: 0.000323\n",
            "Epoch 882: 0.000322\n",
            "Epoch 883: 0.000322\n",
            "Epoch 884: 0.000321\n",
            "Epoch 885: 0.000320\n",
            "Epoch 886: 0.000320\n",
            "Epoch 887: 0.000319\n",
            "Epoch 888: 0.000319\n",
            "Epoch 889: 0.000318\n",
            "Epoch 890: 0.000318\n",
            "Epoch 891: 0.000317\n",
            "Epoch 892: 0.000317\n",
            "Epoch 893: 0.000316\n",
            "Epoch 894: 0.000316\n",
            "Epoch 895: 0.000315\n",
            "Epoch 896: 0.000315\n",
            "Epoch 897: 0.000314\n",
            "Epoch 898: 0.000314\n",
            "Epoch 899: 0.000313\n",
            "Epoch 900: 0.000313\n",
            "Epoch 901: 0.000312\n",
            "Epoch 902: 0.000312\n",
            "Epoch 903: 0.000311\n",
            "Epoch 904: 0.000311\n",
            "Epoch 905: 0.000310\n",
            "Epoch 906: 0.000310\n",
            "Epoch 907: 0.000309\n",
            "Epoch 908: 0.000309\n",
            "Epoch 909: 0.000308\n",
            "Epoch 910: 0.000308\n",
            "Epoch 911: 0.000307\n",
            "Epoch 912: 0.000307\n",
            "Epoch 913: 0.000306\n",
            "Epoch 914: 0.000306\n",
            "Epoch 915: 0.000305\n",
            "Epoch 916: 0.000305\n",
            "Epoch 917: 0.000304\n",
            "Epoch 918: 0.000304\n",
            "Epoch 919: 0.000304\n",
            "Epoch 920: 0.000303\n",
            "Epoch 921: 0.000303\n",
            "Epoch 922: 0.000302\n",
            "Epoch 923: 0.000302\n",
            "Epoch 924: 0.000301\n",
            "Epoch 925: 0.000301\n",
            "Epoch 926: 0.000300\n",
            "Epoch 927: 0.000300\n",
            "Epoch 928: 0.000299\n",
            "Epoch 929: 0.000299\n",
            "Epoch 930: 0.000298\n",
            "Epoch 931: 0.000298\n",
            "Epoch 932: 0.000297\n",
            "Epoch 933: 0.000297\n",
            "Epoch 934: 0.000297\n",
            "Epoch 935: 0.000296\n",
            "Epoch 936: 0.000296\n",
            "Epoch 937: 0.000295\n",
            "Epoch 938: 0.000295\n",
            "Epoch 939: 0.000294\n",
            "Epoch 940: 0.000294\n",
            "Epoch 941: 0.000293\n",
            "Epoch 942: 0.000293\n",
            "Epoch 943: 0.000293\n",
            "Epoch 944: 0.000292\n",
            "Epoch 945: 0.000292\n",
            "Epoch 946: 0.000291\n",
            "Epoch 947: 0.000291\n",
            "Epoch 948: 0.000290\n",
            "Epoch 949: 0.000290\n",
            "Epoch 950: 0.000289\n",
            "Epoch 951: 0.000289\n",
            "Epoch 952: 0.000289\n",
            "Epoch 953: 0.000288\n",
            "Epoch 954: 0.000288\n",
            "Epoch 955: 0.000287\n",
            "Epoch 956: 0.000287\n",
            "Epoch 957: 0.000286\n",
            "Epoch 958: 0.000286\n",
            "Epoch 959: 0.000286\n",
            "Epoch 960: 0.000285\n",
            "Epoch 961: 0.000285\n",
            "Epoch 962: 0.000284\n",
            "Epoch 963: 0.000284\n",
            "Epoch 964: 0.000283\n",
            "Epoch 965: 0.000283\n",
            "Epoch 966: 0.000283\n",
            "Epoch 967: 0.000282\n",
            "Epoch 968: 0.000282\n",
            "Epoch 969: 0.000281\n",
            "Epoch 970: 0.000281\n",
            "Epoch 971: 0.000281\n",
            "Epoch 972: 0.000280\n",
            "Epoch 973: 0.000280\n",
            "Epoch 974: 0.000279\n",
            "Epoch 975: 0.000279\n",
            "Epoch 976: 0.000279\n",
            "Epoch 977: 0.000278\n",
            "Epoch 978: 0.000278\n",
            "Epoch 979: 0.000277\n",
            "Epoch 980: 0.000277\n",
            "Epoch 981: 0.000277\n",
            "Epoch 982: 0.000276\n",
            "Epoch 983: 0.000276\n",
            "Epoch 984: 0.000275\n",
            "Epoch 985: 0.000275\n",
            "Epoch 986: 0.000275\n",
            "Epoch 987: 0.000274\n",
            "Epoch 988: 0.000274\n",
            "Epoch 989: 0.000273\n",
            "Epoch 990: 0.000273\n",
            "Epoch 991: 0.000273\n",
            "Epoch 992: 0.000272\n",
            "Epoch 993: 0.000272\n",
            "Epoch 994: 0.000271\n",
            "Epoch 995: 0.000271\n",
            "Epoch 996: 0.000271\n",
            "Epoch 997: 0.000270\n",
            "Epoch 998: 0.000270\n",
            "Epoch 999: 0.000270\n",
            "[[[ 0.00096813]]\n",
            "\n",
            " [[ 0.97607084]]\n",
            "\n",
            " [[ 0.97761871]]\n",
            "\n",
            " [[-0.00177324]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "tuj7-Jo-IvBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# X is of shape (samples, 28, 28) with each value being [0-255] (greyscale)\n",
        "# Y is of shape (samples,) with values [0-9]\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape to (samples, w*h), EXCEPT\n",
        "# Actually (samples, 1, w*h) for matrix multiplication reasons in first layer backprop\n",
        "# Should add a fix in NN class to handle this internally\n",
        "# Maybe InputLayer class\n",
        "x_train = x_train.reshape((x_train.shape[0], 1, np.prod(x_train.shape[1:]))) / 255\n",
        "x_test = x_test.reshape((x_test.shape[0], 1, np.prod(x_test.shape[1:]))) / 255\n",
        "\n",
        "# Normalization/scaling makes a big difference here(may depend on final layer)\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "act = sigmoid\n",
        "act_d = sigmoid_d\n",
        "\n",
        "model = NeuralNetwork(x_train.shape[-1])\n",
        "model.add_layer(FullyConnectedLayer(100))\n",
        "model.add_layer(ActivationLayer(act, act_d))\n",
        "model.add_layer(FullyConnectedLayer(100))\n",
        "model.add_layer(ActivationLayer(act, act_d))\n",
        "model.add_layer(FullyConnectedLayer(10))\n",
        "model.add_layer(ActivationLayer(act, act_d))\n",
        "model.set_loss_function(mse, mse_d)\n",
        "\n",
        "def calc_loss(model, X, Y, batch_size, loss_func):\n",
        "    error = 0\n",
        "    num_batches = math.ceil(X.shape[0] / batch_size)\n",
        "    for i in range(0, num_batches, 1):\n",
        "        X_batch = X[batch_size*i : batch_size*(i+1)]\n",
        "        Y_batch = Y[batch_size*i : batch_size*(i+1)]\n",
        "        \n",
        "        output = X_batch\n",
        "        for layer in model.layers:\n",
        "            output = layer.forward_prop(output)\n",
        "\n",
        "        Y_batch = Y_batch.reshape(output.shape)\n",
        "        e = loss_func(output, Y_batch) \n",
        "        error += e * X_batch.shape[0]\n",
        "    return error\n",
        "\n",
        "# Error per sample will be same magnitude\n",
        "# Mean of 1 sample will be same as mean of many samples\n",
        "# So with batches, sum of errors will be a factor of batch_size smaller\n",
        "# Multiply final error by batch_size?\n",
        "#print(calc_loss(model, x_train, y_train, 1, mse))\n",
        "#print(calc_loss(model, x_train, y_train, 100, mse))\n",
        "\n",
        "np.random.seed(10)\n",
        "#model.train(x_train[:1000], y_train[0:1000], 0.1, 30, batch_size=1)\n",
        "model.train(x_train[:1000], y_train[0:1000], 0.1*32, 500, batch_size=32)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "n_y = 10\n",
        "predicts = model.predict(x_test[:n_y])\n",
        "for i in range(n_y):\n",
        "    print(\"Predict={:d}  True={:d}\".format(np.argmax(predicts[i]),np.argmax(y_test[i])))\n"
      ],
      "metadata": {
        "id": "6d7st2DGItW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Batch/minibatch updates, input shapes, softmax, crossentropy, refactor, check for others "
      ],
      "metadata": {
        "id": "FGUdZjzokrGZ"
      }
    }
  ]
}